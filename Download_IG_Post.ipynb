{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Download IG Post.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP+rLV1/G+Ziln1vmq/6kDu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danstudiohk/WebScraper/blob/main/Download_IG_Post.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ouskYJba-mKP"
      },
      "outputs": [],
      "source": [
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "import time\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('-headless')\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',options=options)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ig = ['aiahongkongmacau',\n",
        "      'fwd_hk',\n",
        "      'prudential.hongkong',\n",
        "      'chinalifeoversea',\n",
        "      'bowtiehongkong',\n",
        "      'axahongkong',\n",
        "      'hsbc_hk',\n",
        "      'citihongkong',\n",
        "      'stancharthk',\n",
        "      'beahk',\n",
        "      'cncbi.bank'\n",
        "      ]"
      ],
      "metadata": {
        "id": "Y0N8cqVJ-5df"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_ig(ig):\n",
        "    url = f'https://www.picuki.com/profile/{ig}'\n",
        "    wd.get(url)\n",
        "    for j in range(1,20):\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(2)\n",
        "    resp = wd.page_source\n",
        "    soup = BeautifulSoup(resp, 'html.parser')\n",
        "    content = []\n",
        "    #div_all = soup.find_all(\"div\", {\"class\": \"photo\"})\n",
        "    div_all = soup.find_all(\"div\", {\"class\": \"photo-description\"})\n",
        "    for d in div_all:\n",
        "        content.append(d.text)\n",
        "        #a_all = d.find_all(\"a\")\n",
        "        #for a in a_all:\n",
        "        #    img = a.find_all(\"img\")\n",
        "        #    for k in img:\n",
        "        #        content.append(k['alt'])\n",
        "\n",
        "    date = []\n",
        "    div_all = soup.find_all(\"div\", {\"class\": \"time\"})\n",
        "    for d in div_all:\n",
        "        s_all = d.find_all('span')\n",
        "        for s in s_all:\n",
        "            date.append(s.text)\n",
        "\n",
        "    like = []\n",
        "    div_all = soup.find_all(\"div\", {\"class\": \"likes_photo\"})\n",
        "    for d in div_all:\n",
        "        like.append(d.text.replace(\"\\n\",\"\"))\n",
        "\n",
        "    comment = []\n",
        "    div_all = soup.find_all(\"div\", {\"class\": \"comments_photo\"})\n",
        "    for d in div_all:\n",
        "        comment.append(d.text.replace(\"\\n\",\"\"))\n",
        "\n",
        "    df = pd.DataFrame(list(zip(date, content, like, comment)),\n",
        "                columns =['ig_post_date', 'ig_content', 'ig_like_cnt', 'ig_comment_cnt'])\n",
        "    df['ig_page'] = ig\n",
        "    df = df[['ig_page','ig_post_date','ig_content', 'ig_like_cnt', 'ig_comment_cnt']]\n",
        "    return df\n",
        "\n",
        "df_aia = scrape_ig(ig[0])\n",
        "df_fwd = scrape_ig(ig[1])\n",
        "df_pru = scrape_ig(ig[2])\n",
        "df_chinalife = scrape_ig(ig[3])\n",
        "df_bowtie = scrape_ig(ig[4])\n",
        "df_axa = scrape_ig(ig[5])\n",
        "df_hsbc = scrape_ig(ig[6])\n",
        "df_citi = scrape_ig(ig[7])\n",
        "df_scb = scrape_ig(ig[8])\n",
        "df_bea = scrape_ig(ig[9])\n",
        "df_citic = scrape_ig(ig[10])\n",
        "\n",
        "df_all = pd.concat([df_aia, df_fwd, df_pru, df_chinalife, df_bowtie, df_axa, df_hsbc, df_citi, df_scb, df_bea, df_citic])\n",
        "df_all\n",
        "\n",
        "from datetime import datetime\n",
        "d = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
      ],
      "metadata": {
        "id": "ptgY_hRX-zjA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}