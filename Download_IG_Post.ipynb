{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Download IG Post.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1lnPLv6StNZWlAQtOCl1PnungdP42t6a_",
      "authorship_tag": "ABX9TyOgd9HckQY7sT5H3E1b9FJa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danstudiohk/WebScraper/blob/main/Download_IG_Post.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouskYJba-mKP",
        "outputId": "79db2d76-5298-44f0-f838-cffa36453763"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 39 not upgraded.\n",
            "Need to get 88.3 MB of archives.\n",
            "After this operation, 294 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 99.0.4844.84-0ubuntu0.18.04.1 [1,142 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 99.0.4844.84-0ubuntu0.18.04.1 [77.7 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 99.0.4844.84-0ubuntu0.18.04.1 [4,397 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 99.0.4844.84-0ubuntu0.18.04.1 [5,092 kB]\n",
            "Fetched 88.3 MB in 2s (43.2 MB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 155455 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_99.0.4844.84-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (99.0.4844.84-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_99.0.4844.84-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (99.0.4844.84-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_99.0.4844.84-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (99.0.4844.84-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_99.0.4844.84-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (99.0.4844.84-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (99.0.4844.84-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser (99.0.4844.84-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (99.0.4844.84-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (99.0.4844.84-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n",
            "Collecting selenium\n",
            "  Downloading selenium-4.1.3-py3-none-any.whl (968 kB)\n",
            "\u001b[K     |████████████████████████████████| 968 kB 5.2 MB/s \n",
            "\u001b[?25hCollecting trio-websocket~=0.9\n",
            "  Downloading trio_websocket-0.9.2-py3-none-any.whl (16 kB)\n",
            "Collecting trio~=0.17\n",
            "  Downloading trio-0.20.0-py3-none-any.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 7.8 MB/s \n",
            "\u001b[?25hCollecting urllib3[secure,socks]~=1.26\n",
            "  Downloading urllib3-1.26.9-py2.py3-none-any.whl (138 kB)\n",
            "\u001b[K     |████████████████████████████████| 138 kB 39.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (21.4.0)\n",
            "Collecting outcome\n",
            "  Downloading outcome-1.1.0-py2.py3-none-any.whl (9.7 kB)\n",
            "Collecting async-generator>=1.9\n",
            "  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.4.0)\n",
            "Collecting sniffio\n",
            "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.7/dist-packages (from trio~=0.17->selenium) (2.10)\n",
            "Collecting wsproto>=0.14\n",
            "  Downloading wsproto-1.1.0-py3-none-any.whl (24 kB)\n",
            "Collecting pyOpenSSL>=0.14\n",
            "  Downloading pyOpenSSL-22.0.0-py2.py3-none-any.whl (55 kB)\n",
            "\u001b[K     |████████████████████████████████| 55 kB 3.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (2021.10.8)\n",
            "Collecting cryptography>=1.3.4\n",
            "  Downloading cryptography-36.0.2-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.6 MB 35.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from urllib3[secure,socks]~=1.26->selenium) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=1.3.4->urllib3[secure,socks]~=1.26->selenium) (2.21)\n",
            "Collecting h11<1,>=0.9.0\n",
            "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from h11<1,>=0.9.0->wsproto>=0.14->trio-websocket~=0.9->selenium) (4.1.1)\n",
            "Installing collected packages: sniffio, outcome, h11, cryptography, async-generator, wsproto, urllib3, trio, pyOpenSSL, trio-websocket, selenium\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.9 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed async-generator-1.10 cryptography-36.0.2 h11-0.13.0 outcome-1.1.0 pyOpenSSL-22.0.0 selenium-4.1.3 sniffio-1.2.0 trio-0.20.0 trio-websocket-0.9.2 urllib3-1.26.9 wsproto-1.1.0\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "!pip install selenium\n",
        "!pip install beautifulsoup4\n",
        "\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "options = webdriver.ChromeOptions()\n",
        "options.add_argument('-headless')\n",
        "options.add_argument('-no-sandbox')\n",
        "options.add_argument('-disable-dev-shm-usage')\n",
        "wd = webdriver.Chrome('chromedriver',options=options)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ig = ['aiahongkongmacau',\n",
        "      'fwd_hk',\n",
        "      'prudential.hongkong',\n",
        "      'chinalifeoversea',\n",
        "      'bowtiehongkong',\n",
        "      'axahongkong',\n",
        "      'hsbc_hk',\n",
        "      'citihongkong',\n",
        "      'stancharthk',\n",
        "      'beahk',\n",
        "      'cncbi.bank'\n",
        "      ]"
      ],
      "metadata": {
        "id": "Y0N8cqVJ-5df"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_ig(ig):\n",
        "    url = f'https://www.picuki.com/profile/{ig}'\n",
        "    wd.get(url)\n",
        "    LoopCnt = 20\n",
        "    for j in range(1, LoopCnt):\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(2)\n",
        "    resp = wd.page_source\n",
        "    soup = BeautifulSoup(resp, 'html.parser')\n",
        "    content = []\n",
        "\n",
        "    div_all = soup.find_all(\"div\", {\"class\": \"photo-description\"})\n",
        "    for d in div_all:\n",
        "        content.append(d.text)\n",
        "\n",
        "    date = []\n",
        "    div_all = soup.find_all(\"div\", {\"class\": \"time\"})\n",
        "    for d in div_all:\n",
        "        s_all = d.find_all('span')\n",
        "        for s in s_all:\n",
        "            date.append(s.text)\n",
        "\n",
        "    like = []\n",
        "    div_all = soup.find_all(\"div\", {\"class\": \"likes_photo\"})\n",
        "    for d in div_all:\n",
        "        like.append(d.text.replace(\"\\n\",\"\"))\n",
        "\n",
        "    comment = []\n",
        "    div_all = soup.find_all(\"div\", {\"class\": \"comments_photo\"})\n",
        "    for d in div_all:\n",
        "        comment.append(d.text.replace(\"\\n\",\"\"))\n",
        "\n",
        "    df = pd.DataFrame(list(zip(date, content, like, comment)),\n",
        "                columns =['ig_post_date', 'ig_content', 'ig_like_cnt', 'ig_comment_cnt'])\n",
        "    df['ig_page'] = ig\n",
        "    df = df[['ig_page','ig_post_date','ig_content', 'ig_like_cnt', 'ig_comment_cnt']]\n",
        "    return df\n",
        "\n",
        "df_aia = scrape_ig(ig[0])\n",
        "df_fwd = scrape_ig(ig[1])\n",
        "df_pru = scrape_ig(ig[2])\n",
        "df_chinalife = scrape_ig(ig[3])\n",
        "df_bowtie = scrape_ig(ig[4])\n",
        "df_axa = scrape_ig(ig[5])\n",
        "df_hsbc = scrape_ig(ig[6])\n",
        "df_citi = scrape_ig(ig[7])\n",
        "df_scb = scrape_ig(ig[8])\n",
        "df_bea = scrape_ig(ig[9])\n",
        "df_citic = scrape_ig(ig[10])\n",
        "\n",
        "df_all = pd.concat([df_aia, df_fwd, df_pru, df_chinalife, df_bowtie, df_axa, df_hsbc, df_citi, df_scb, df_bea, df_citic])\n",
        "df_all['ig_extract_date'] = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "\n",
        "df_all.to_csv('/content/drive/MyDrive/HSBC Market Intelligence/ig'+ datetime.now().strftime(\"%Y%m%d%H%M%S\") + '.csv')\n",
        "df_all.to_csv('/content/drive/MyDrive/HSBC Market Intelligence/ig.csv')"
      ],
      "metadata": {
        "id": "ptgY_hRX-zjA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MraY9AHoC1xb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v4uhOiEhD00L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}